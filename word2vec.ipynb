{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c9e1d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\150ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\150ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\150ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy as sp\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "import scikitplot as skplt\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "STOPWORDS = set(stopwords.words('russian'))\n",
    "#list of special characters.You can use regular expressions too\n",
    "\n",
    "# importing the PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#importing the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "# start text processing with vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65f5fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c53b1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ef7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = SnowballStemmer(language=\"russian\")\n",
    "lemmatizer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307aa3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Досудебное расследование по факту покупки ЕНПФ...</td>\n",
       "      <td>1945</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Медики рассказали о состоянии пострадавшего му...</td>\n",
       "      <td>1957</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Прошел почти год, как железнодорожным оператор...</td>\n",
       "      <td>1969</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>По итогам 12 месяцев 2016 года на территории р...</td>\n",
       "      <td>1973</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Астана. 21 ноября. Kazakhstan Today - Агентств...</td>\n",
       "      <td>1975</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8258</th>\n",
       "      <td>Как мы писали еще весной, для увеличения сбыта...</td>\n",
       "      <td>10312</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8259</th>\n",
       "      <td>Но молодой министр национальной экономики Биши...</td>\n",
       "      <td>10313</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8260</th>\n",
       "      <td>\\n \\nВ ЕНПФ назначен новый председатель правле...</td>\n",
       "      <td>10314</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8261</th>\n",
       "      <td>В Алматы у отделения банка произошло нападение...</td>\n",
       "      <td>10315</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8262</th>\n",
       "      <td>НПП РК «Атамекен» предлагает создать Националь...</td>\n",
       "      <td>10316</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8263 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     id sentiment\n",
       "0     Досудебное расследование по факту покупки ЕНПФ...   1945  negative\n",
       "1     Медики рассказали о состоянии пострадавшего му...   1957  negative\n",
       "2     Прошел почти год, как железнодорожным оператор...   1969  negative\n",
       "3     По итогам 12 месяцев 2016 года на территории р...   1973  negative\n",
       "4     Астана. 21 ноября. Kazakhstan Today - Агентств...   1975  negative\n",
       "...                                                 ...    ...       ...\n",
       "8258  Как мы писали еще весной, для увеличения сбыта...  10312  positive\n",
       "8259  Но молодой министр национальной экономики Биши...  10313  negative\n",
       "8260  \\n \\nВ ЕНПФ назначен новый председатель правле...  10314   neutral\n",
       "8261  В Алматы у отделения банка произошло нападение...  10315  negative\n",
       "8262  НПП РК «Атамекен» предлагает создать Националь...  10316   neutral\n",
       "\n",
       "[8263 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_json('train.json')\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737f5bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Как сообщает пресс-служба акимата Алматы, для ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Казахстанские авиакомпании перевозят 250 тысяч...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>На состоявшемся под председательством Касым-Жо...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В ОАЭ состоялись переговоры между казахстанско...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 вагонов грузового поезда сошли с путей в Во...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>На официальной странице общественного движения...</td>\n",
       "      <td>2079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>официальный курс – 330,55 тенге за Доллар США ...</td>\n",
       "      <td>2083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>«Базовая ставка, которая сейчас составляет 12%...</td>\n",
       "      <td>2084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>На начальном этапе за неоплату парковки на при...</td>\n",
       "      <td>2087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>Российский авторынок в октябре снизил темпы па...</td>\n",
       "      <td>2088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2056 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    id\n",
       "0     Как сообщает пресс-служба акимата Алматы, для ...     0\n",
       "1     Казахстанские авиакомпании перевозят 250 тысяч...     1\n",
       "2     На состоявшемся под председательством Касым-Жо...     2\n",
       "3     В ОАЭ состоялись переговоры между казахстанско...     3\n",
       "4     12 вагонов грузового поезда сошли с путей в Во...     4\n",
       "...                                                 ...   ...\n",
       "2051  На официальной странице общественного движения...  2079\n",
       "2052  официальный курс – 330,55 тенге за Доллар США ...  2083\n",
       "2053  «Базовая ставка, которая сейчас составляет 12%...  2084\n",
       "2054  На начальном этапе за неоплату парковки на при...  2087\n",
       "2055  Российский авторынок в октябре снизил темпы па...  2088\n",
       "\n",
       "[2056 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_json('test.json')\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbe3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]+|[\\d]+', r'',text).strip()\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub(' ', text)\n",
    "#     text = re.sub(r'[^0-9a-zA-Z]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = \" \".join(word for word in text.split() if word not in STOPWORDS)\n",
    "    text = \" \".join(ps.stem(lemmatizer.normal_forms(word)[0]) for word in text.split())\n",
    "    return text\n",
    "  \n",
    "data_train['clean_text'] = data_train['text'].apply(clean_text)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65df24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to get rid of stopwords present in the messages\n",
    "def message_text_process(mess):\n",
    "    # Check characters to see if there are punctuations \n",
    "    no_punctuation=[char for char in mess if char not in string.punctuation]\n",
    "    # now form the sentence\n",
    "    no_punctuation=''.join(no_punctuation)\n",
    "    # Now eliminate any stopwords\n",
    "    return[word for word in no_punctuation.split() if word.lower() not in stopwords.words('english') and stopwords.words('russian')]\n",
    "\n",
    "# to verify that function is working\n",
    "# data_train['text'].head().apply(message_text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbaebdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Досудебное, расследование, по, факту, покупки...\n",
       "1       [Медики, рассказали, о, состоянии, пострадавше...\n",
       "2       [Прошел, почти, год, как, железнодорожным, опе...\n",
       "3       [По, итогам, 12, месяцев, 2016, года, на, терр...\n",
       "4       [Астана, 21, ноября, Kazakhstan, Today, Агентс...\n",
       "                              ...                        \n",
       "8258    [Как, мы, писали, еще, весной, для, увеличения...\n",
       "8259    [Но, молодой, министр, национальной, экономики...\n",
       "8260    [В, ЕНПФ, назначен, новый, председатель, правл...\n",
       "8261    [В, Алматы, у, отделения, банка, произошло, на...\n",
       "8262    [НПП, РК, «Атамекен», предлагает, создать, Нац...\n",
       "Name: text, Length: 8263, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data = data_train['text'].apply(message_text_process)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f3f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Досудебное, расследование, по, факту, покупки...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Медики, рассказали, о, состоянии, пострадавше...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Прошел, почти, год, как, железнодорожным, опе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[По, итогам, 12, месяцев, 2016, года, на, терр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Астана, 21, ноября, Kazakhstan, Today, Агентс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8258</th>\n",
       "      <td>[Как, мы, писали, еще, весной, для, увеличения...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8259</th>\n",
       "      <td>[Но, молодой, министр, национальной, экономики...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8260</th>\n",
       "      <td>[В, ЕНПФ, назначен, новый, председатель, правл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8261</th>\n",
       "      <td>[В, Алматы, у, отделения, банка, произошло, на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8262</th>\n",
       "      <td>[НПП, РК, «Атамекен», предлагает, создать, Нац...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8263 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     [Досудебное, расследование, по, факту, покупки...\n",
       "1     [Медики, рассказали, о, состоянии, пострадавше...\n",
       "2     [Прошел, почти, год, как, железнодорожным, опе...\n",
       "3     [По, итогам, 12, месяцев, 2016, года, на, терр...\n",
       "4     [Астана, 21, ноября, Kazakhstan, Today, Агентс...\n",
       "...                                                 ...\n",
       "8258  [Как, мы, писали, еще, весной, для, увеличения...\n",
       "8259  [Но, молодой, министр, национальной, экономики...\n",
       "8260  [В, ЕНПФ, назначен, новый, председатель, правл...\n",
       "8261  [В, Алматы, у, отделения, банка, произошло, на...\n",
       "8262  [НПП, РК, «Атамекен», предлагает, создать, Нац...\n",
       "\n",
       "[8263 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39e5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv('data_train_clean_text.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a87ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_train_message_text_process.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d57d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('data_train_clean_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e257a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "#     # Check characters to see if there are punctuations \n",
    "#     no_punctuation=[char for char in mess if char not in string.punctuation]\n",
    "#     # now form the sentence\n",
    "    no_punctuation=''.join(mess)\n",
    "    # Now eliminate any stopwords\n",
    "    return[word.lower() for word in no_punctuation.split()]\n",
    "#     return no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c095ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [досудебн, расследован, факт, покупк, енпф, па...\n",
       "1       [медик, рассказа, состоян, пострада, мужчин, к...\n",
       "2       [пройт, год, железнодорожн, оператор, запрет, ...\n",
       "3       [итог, месяц, год, территор, республик, выпуст...\n",
       "4       [аста, ноябр, kazakhstan, today, агентств, рк,...\n",
       "                              ...                        \n",
       "8258    [писа, весн, увеличен, сбыт, такж, избеган, пе...\n",
       "8259    [молод, министр, национальн, экономик, бишимба...\n",
       "8260    [енпф, назнач, нов, председател, правлен, един...\n",
       "8261    [алмат, отделен, банк, произойт, нападен, стре...\n",
       "8262    [нпп, рк, атамек, предлага, созда, национальн,...\n",
       "Name: clean_text, Length: 8263, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data1['clean_text'].apply(text_process)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65ff9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0398f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=10,\n",
    "    window=2,\n",
    "    vector_size=300,\n",
    "    negative=10,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    sample=6e-5,\n",
    "    sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9ebd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae82ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50092135, 90434190)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(data, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d1f9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3bcbc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('хх', 0.331299364566803),\n",
       " ('переплат', 0.3288971781730652),\n",
       " ('токсиколог', 0.32131096720695496),\n",
       " ('xx', 0.3181544840335846),\n",
       " ('барачн', 0.3111740052700043),\n",
       " ('kulanshi', 0.30679190158843994),\n",
       " ('газовоздушн', 0.3040841519832611),\n",
       " ('пятнадца', 0.303924560546875),\n",
       " ('приблизительн', 0.2980707287788391),\n",
       " ('резервуарн', 0.29701241850852966)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"х\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "548fec75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14416, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57b269c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01300017,  0.12605561,  0.01273356, ...,  0.01182407,\n",
       "         0.1165512 , -0.09134565],\n",
       "       [ 0.00589335,  0.2540365 , -0.05305502, ..., -0.13445063,\n",
       "        -0.21756735, -0.14814591],\n",
       "       [-0.02949037,  0.04986649, -0.04000168, ..., -0.03994336,\n",
       "         0.01483119, -0.00037922],\n",
       "       ...,\n",
       "       [ 0.04874252, -0.23005418,  0.00988154, ..., -0.00546055,\n",
       "        -0.37245917,  0.10539123],\n",
       "       [ 0.14578563,  0.37379646,  0.33995473, ..., -0.18594214,\n",
       "         0.18681559,  0.14370292],\n",
       "       [-0.05281521,  0.34247145,  0.05633693, ...,  0.130997  ,\n",
       "        -0.10204629,  0.00074941]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e121539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "filename1 = 'w2v_model.txt'\n",
    "w2v_model.wv.save_word2vec_format(filename1, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f5cc6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_i = {}\n",
    "f = open(os.path.join('', 'w2v_model.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_i[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "618ce7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "906f5ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a8d82af701480b9cc59d705b304550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8263, 0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "docs_vectors2 = pd.DataFrame() # creating empty final dataframe\n",
    "# stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
    "for doc in tqdm(data):#.str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
    "    temp2 = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "    for word in doc: # looping through each word of a single document and spliting through space\n",
    "#         if word not in STOPWORDS: # if word is not present in stopwords then (try)\n",
    "        try:\n",
    "                    word_vec2 = embeddings_i[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                    temp2 = temp1.append(pd.Series(word_vec2), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "        except:\n",
    "                    pass\n",
    "    doc_vector2 = temp2.mean() # take the average of each column(w0, w1, w2,........w300)\n",
    "    docs_vectors2 = docs_vectors2.append(doc_vector2, ignore_index = True) # append each document value to the final dataframe\n",
    "docs_vectors2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87329bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6dd7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87740fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# review_lines = list()\n",
    "# lines = data.values.tolist()\n",
    "\n",
    "# for line in lines:   \n",
    "#     tokens = word_tokenize(line)\n",
    "#     # convert to lower case\n",
    "#     tokens = [w.lower() for w in tokens]\n",
    "#     # remove punctuation from each word    \n",
    "#     table = str.maketrans('', '', string.punctuation)\n",
    "#     stripped = [w.translate(table) for w in tokens]\n",
    "#     # remove remaining tokens that are not alphabetic\n",
    "#     words = [word for word in stripped if word.isalpha()]\n",
    "#     # filter out stop words    \n",
    "#     stop_words = set(stopwords.words('russian'))\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "#     review_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4339a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8263"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(review_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8854f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349d2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=data.values.tolist(), vector_size=EMBEDDING_DIM, window=1, workers=4, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76214b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62461"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88f50c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "filename = 'russian_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "047832f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'russian_embedding_word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccd0f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_reviews = data_train['text'].values\n",
    "# max_length = max([len(s.split()) for s in total_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0cb7f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48146\n"
     ]
    }
   ],
   "source": [
    "# print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56eafa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed405c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c74915434134e8faf4efa412cddcec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "docs_vectors1 = pd.DataFrame() # creating empty final dataframe\n",
    "# stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
    "for doc in tqdm(data):#.str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
    "    temp1 = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "    for word in doc: # looping through each word of a single document and spliting through space\n",
    "#         if word not in STOPWORDS: # if word is not present in stopwords then (try)\n",
    "        try:\n",
    "                    word_vec1 = embeddings_index[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                    temp1 = temp1.append(pd.Series(word_vec1), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "        except:\n",
    "                    pass\n",
    "    doc_vector1 = temp1.mean() # take the average of each column(w0, w1, w2,........w300)\n",
    "    docs_vectors1 = docs_vectors1.append(doc_vector1, ignore_index = True) # append each document value to the final dataframe\n",
    "docs_vectors1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae79397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "url = \"model.bin\"\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c81de6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = ' '.join(embeddings.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b00c1eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0404252cb6a242278e7311efec7f3bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8263, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "# stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
    "for doc in tqdm(data):#.str.lower().str.replace('[^a-z ]', ''): # looping through each document and cleaning it\n",
    "    temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "    for word in doc: # looping through each word of a single document and spliting through space\n",
    "#         if word not in STOPWORDS: # if word is not present in stopwords then (try)\n",
    "            try:\n",
    "                        e = r'{}\\w+'.format(word)\n",
    "                        prog = re.compile(e)\n",
    "                        r = re.search(prog, xxx)\n",
    "                        t = r.group(0)\n",
    "           \n",
    "                        word_vec = embeddings[t] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                        temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "            except:\n",
    "                        pass\n",
    "    doc_vector = temp.mean() # take the average of each column(w0, w1, w2,........w300)\n",
    "    docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "docs_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "434e9f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031685</td>\n",
       "      <td>0.178949</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>-0.135201</td>\n",
       "      <td>-0.060891</td>\n",
       "      <td>0.134090</td>\n",
       "      <td>-0.069089</td>\n",
       "      <td>-0.120070</td>\n",
       "      <td>0.032332</td>\n",
       "      <td>-0.242677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068924</td>\n",
       "      <td>-0.045049</td>\n",
       "      <td>0.077332</td>\n",
       "      <td>0.143879</td>\n",
       "      <td>-0.093730</td>\n",
       "      <td>-0.151954</td>\n",
       "      <td>-0.086196</td>\n",
       "      <td>0.119717</td>\n",
       "      <td>0.186446</td>\n",
       "      <td>0.122009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.171886</td>\n",
       "      <td>-0.067648</td>\n",
       "      <td>-0.059812</td>\n",
       "      <td>-0.138297</td>\n",
       "      <td>0.147603</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>-0.218301</td>\n",
       "      <td>0.074575</td>\n",
       "      <td>-0.205155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>-0.005971</td>\n",
       "      <td>0.080198</td>\n",
       "      <td>0.072171</td>\n",
       "      <td>0.038017</td>\n",
       "      <td>-0.127977</td>\n",
       "      <td>-0.036574</td>\n",
       "      <td>0.136970</td>\n",
       "      <td>0.072631</td>\n",
       "      <td>0.127916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.005910</td>\n",
       "      <td>-0.029268</td>\n",
       "      <td>-0.117138</td>\n",
       "      <td>-0.076510</td>\n",
       "      <td>0.156052</td>\n",
       "      <td>0.016359</td>\n",
       "      <td>-0.190923</td>\n",
       "      <td>0.097740</td>\n",
       "      <td>-0.199994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042932</td>\n",
       "      <td>-0.122284</td>\n",
       "      <td>0.049808</td>\n",
       "      <td>0.223850</td>\n",
       "      <td>-0.132330</td>\n",
       "      <td>-0.134603</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>0.064367</td>\n",
       "      <td>0.022614</td>\n",
       "      <td>0.066349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.052623</td>\n",
       "      <td>0.024831</td>\n",
       "      <td>-0.014060</td>\n",
       "      <td>-0.137594</td>\n",
       "      <td>-0.051393</td>\n",
       "      <td>0.056387</td>\n",
       "      <td>-0.019196</td>\n",
       "      <td>-0.137196</td>\n",
       "      <td>0.145147</td>\n",
       "      <td>-0.241803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089164</td>\n",
       "      <td>-0.088749</td>\n",
       "      <td>0.049736</td>\n",
       "      <td>0.263677</td>\n",
       "      <td>-0.156651</td>\n",
       "      <td>-0.016040</td>\n",
       "      <td>-0.120103</td>\n",
       "      <td>0.169930</td>\n",
       "      <td>0.086296</td>\n",
       "      <td>0.087752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022835</td>\n",
       "      <td>0.096034</td>\n",
       "      <td>-0.044885</td>\n",
       "      <td>-0.027258</td>\n",
       "      <td>-0.043773</td>\n",
       "      <td>0.082485</td>\n",
       "      <td>-0.013675</td>\n",
       "      <td>-0.225652</td>\n",
       "      <td>0.117113</td>\n",
       "      <td>-0.168474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048638</td>\n",
       "      <td>-0.102031</td>\n",
       "      <td>0.092301</td>\n",
       "      <td>0.159423</td>\n",
       "      <td>-0.072504</td>\n",
       "      <td>-0.125476</td>\n",
       "      <td>-0.024196</td>\n",
       "      <td>0.136839</td>\n",
       "      <td>0.169404</td>\n",
       "      <td>0.022702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8258</th>\n",
       "      <td>-0.009181</td>\n",
       "      <td>0.005057</td>\n",
       "      <td>-0.042632</td>\n",
       "      <td>-0.082400</td>\n",
       "      <td>-0.086237</td>\n",
       "      <td>0.080103</td>\n",
       "      <td>0.031975</td>\n",
       "      <td>-0.170380</td>\n",
       "      <td>0.189781</td>\n",
       "      <td>-0.230101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164937</td>\n",
       "      <td>-0.141198</td>\n",
       "      <td>-0.009332</td>\n",
       "      <td>0.225098</td>\n",
       "      <td>-0.105520</td>\n",
       "      <td>-0.087568</td>\n",
       "      <td>-0.068922</td>\n",
       "      <td>0.134096</td>\n",
       "      <td>0.065158</td>\n",
       "      <td>0.041643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8259</th>\n",
       "      <td>0.048767</td>\n",
       "      <td>0.059226</td>\n",
       "      <td>-0.008510</td>\n",
       "      <td>-0.077230</td>\n",
       "      <td>-0.068700</td>\n",
       "      <td>0.131523</td>\n",
       "      <td>0.066039</td>\n",
       "      <td>-0.185712</td>\n",
       "      <td>0.076746</td>\n",
       "      <td>-0.197847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122072</td>\n",
       "      <td>-0.081235</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>0.206319</td>\n",
       "      <td>-0.109283</td>\n",
       "      <td>-0.063879</td>\n",
       "      <td>-0.032915</td>\n",
       "      <td>0.148445</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.032009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8260</th>\n",
       "      <td>-0.030871</td>\n",
       "      <td>0.210113</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>-0.050671</td>\n",
       "      <td>0.038129</td>\n",
       "      <td>0.044490</td>\n",
       "      <td>-0.133662</td>\n",
       "      <td>-0.182833</td>\n",
       "      <td>-0.081432</td>\n",
       "      <td>-0.128037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102866</td>\n",
       "      <td>0.100666</td>\n",
       "      <td>-0.084105</td>\n",
       "      <td>0.060187</td>\n",
       "      <td>-0.109676</td>\n",
       "      <td>0.032487</td>\n",
       "      <td>-0.083420</td>\n",
       "      <td>0.083041</td>\n",
       "      <td>0.218849</td>\n",
       "      <td>0.003763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8261</th>\n",
       "      <td>0.021308</td>\n",
       "      <td>0.170105</td>\n",
       "      <td>-0.091837</td>\n",
       "      <td>-0.090477</td>\n",
       "      <td>-0.070167</td>\n",
       "      <td>0.187116</td>\n",
       "      <td>0.019334</td>\n",
       "      <td>-0.259912</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>-0.184171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046713</td>\n",
       "      <td>-0.109891</td>\n",
       "      <td>0.076076</td>\n",
       "      <td>0.100614</td>\n",
       "      <td>0.017036</td>\n",
       "      <td>-0.092438</td>\n",
       "      <td>-0.021866</td>\n",
       "      <td>0.180605</td>\n",
       "      <td>0.055763</td>\n",
       "      <td>0.139543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8262</th>\n",
       "      <td>0.006831</td>\n",
       "      <td>0.015384</td>\n",
       "      <td>-0.078626</td>\n",
       "      <td>-0.089112</td>\n",
       "      <td>-0.059625</td>\n",
       "      <td>0.104977</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>-0.237927</td>\n",
       "      <td>0.091993</td>\n",
       "      <td>-0.184528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093477</td>\n",
       "      <td>-0.065562</td>\n",
       "      <td>0.098507</td>\n",
       "      <td>0.188154</td>\n",
       "      <td>-0.090345</td>\n",
       "      <td>-0.084210</td>\n",
       "      <td>-0.038751</td>\n",
       "      <td>0.112534</td>\n",
       "      <td>0.123820</td>\n",
       "      <td>0.028266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8263 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.031685  0.178949  0.009137 -0.135201 -0.060891  0.134090 -0.069089   \n",
       "1     0.000640  0.171886 -0.067648 -0.059812 -0.138297  0.147603  0.000309   \n",
       "2     0.002386  0.005910 -0.029268 -0.117138 -0.076510  0.156052  0.016359   \n",
       "3     0.052623  0.024831 -0.014060 -0.137594 -0.051393  0.056387 -0.019196   \n",
       "4     0.022835  0.096034 -0.044885 -0.027258 -0.043773  0.082485 -0.013675   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8258 -0.009181  0.005057 -0.042632 -0.082400 -0.086237  0.080103  0.031975   \n",
       "8259  0.048767  0.059226 -0.008510 -0.077230 -0.068700  0.131523  0.066039   \n",
       "8260 -0.030871  0.210113  0.004565 -0.050671  0.038129  0.044490 -0.133662   \n",
       "8261  0.021308  0.170105 -0.091837 -0.090477 -0.070167  0.187116  0.019334   \n",
       "8262  0.006831  0.015384 -0.078626 -0.089112 -0.059625  0.104977  0.002330   \n",
       "\n",
       "           7         8         9    ...       290       291       292  \\\n",
       "0    -0.120070  0.032332 -0.242677  ...  0.068924 -0.045049  0.077332   \n",
       "1    -0.218301  0.074575 -0.205155  ...  0.066938 -0.005971  0.080198   \n",
       "2    -0.190923  0.097740 -0.199994  ...  0.042932 -0.122284  0.049808   \n",
       "3    -0.137196  0.145147 -0.241803  ...  0.089164 -0.088749  0.049736   \n",
       "4    -0.225652  0.117113 -0.168474  ...  0.048638 -0.102031  0.092301   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8258 -0.170380  0.189781 -0.230101  ...  0.164937 -0.141198 -0.009332   \n",
       "8259 -0.185712  0.076746 -0.197847  ...  0.122072 -0.081235  0.005561   \n",
       "8260 -0.182833 -0.081432 -0.128037  ...  0.102866  0.100666 -0.084105   \n",
       "8261 -0.259912  0.014777 -0.184171  ...  0.046713 -0.109891  0.076076   \n",
       "8262 -0.237927  0.091993 -0.184528  ...  0.093477 -0.065562  0.098507   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0     0.143879 -0.093730 -0.151954 -0.086196  0.119717  0.186446  0.122009  \n",
       "1     0.072171  0.038017 -0.127977 -0.036574  0.136970  0.072631  0.127916  \n",
       "2     0.223850 -0.132330 -0.134603  0.073164  0.064367  0.022614  0.066349  \n",
       "3     0.263677 -0.156651 -0.016040 -0.120103  0.169930  0.086296  0.087752  \n",
       "4     0.159423 -0.072504 -0.125476 -0.024196  0.136839  0.169404  0.022702  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "8258  0.225098 -0.105520 -0.087568 -0.068922  0.134096  0.065158  0.041643  \n",
       "8259  0.206319 -0.109283 -0.063879 -0.032915  0.148445  0.043832  0.032009  \n",
       "8260  0.060187 -0.109676  0.032487 -0.083420  0.083041  0.218849  0.003763  \n",
       "8261  0.100614  0.017036 -0.092438 -0.021866  0.180605  0.055763  0.139543  \n",
       "8262  0.188154 -0.090345 -0.084210 -0.038751  0.112534  0.123820  0.028266  \n",
       "\n",
       "[8263 rows x 300 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2743b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors['Sentiment'] = data_train['sentiment']\n",
    "docs_vectors = docs_vectors.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba790645",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors['Sentiment'] = docs_vectors['Sentiment'].map(lambda x: 0 if x == 'negative' else 1 if x == 'positive' else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c596380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(docs_vectors).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a3b38a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6610, 300), (6610,), (1653, 300), (1653,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(docs_vectors.drop('Sentiment', axis = 1),\n",
    "                                                   docs_vectors['Sentiment'],\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state = 1)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8bb790a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6110102843315185"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = AdaBoostClassifier(n_estimators=800, random_state = 1)\n",
    "model1.fit(train_x, train_y)\n",
    "test_pred1 = model1.predict(test_x)\n",
    "accuracy_score(test_y, test_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6476f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"model_w2v_AdaBoostClassifier.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93ef9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900159c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
